{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6aes4mfn+X0UhKHd+8rcs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Energy-Consumption-Forecasting/blob/main/Forecasting_Daily_Energy_Demand_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtZwK67hI1e3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from Kaggle\n",
        "df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"jeanmidev/smart-meters-in-london\",\n",
        "    \"daily_dataset.csv\",\n",
        ")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "R5u_RPcTKWKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop LCLid column if it exists\n",
        "df.drop(columns=['LCLid'], errors='ignore', inplace=True)"
      ],
      "metadata": {
        "id": "16kxt7WKLbwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "qX0Sp5pmLdTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'day' column to datetime and set index\n",
        "df['day'] = pd.to_datetime(df['day'])\n",
        "df.set_index('day', inplace=True)\n",
        "\n",
        "# Selecting energy_sum as target variable\n",
        "df_daily = df[['energy_sum']]\n",
        "\n",
        "\n",
        "# Normalize data\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df_daily)\n",
        "\n",
        "# Prepare dataset for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 30  # Using past 30 days to predict the next day\n",
        "X, y = create_sequences(df_scaled, seq_length)\n",
        "\n",
        "# Split data into train and test sets\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Build LSTM Model\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(seq_length, X.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(25, activation='relu'),\n",
        "    Dense(X.shape[2])\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=16)\n"
      ],
      "metadata": {
        "id": "OlG4RV3eKkuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_test_inv = scaler.inverse_transform(y_test)\n",
        "y_pred_inv = scaler.inverse_transform(y_pred)\n",
        "\n",
        "# Plot actual vs predicted\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(y_test_inv, label='Actual Consumption (energy_sum)')\n",
        "plt.plot(y_pred_inv, label='Predicted Consumption (energy_sum)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UqhkY32PKuxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('energy_forecast_model.h5')"
      ],
      "metadata": {
        "id": "udrJeMuMM8LO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}